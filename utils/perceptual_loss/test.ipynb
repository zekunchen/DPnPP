{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建张量并启用梯度跟踪\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# 计算 y 对 x 的梯度\n",
    "grad = torch.autograd.grad(y, x)\n",
    "print(grad)  # 输出: (tensor(4.0),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义神经网络模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 定义网络结构，这里只是一个简单的示例\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# 定义损失函数\n",
    "def loss_function(output, target, regularization_term, eta):\n",
    "    reconstruction_loss = torch.norm(output - target)**2\n",
    "    total_loss = reconstruction_loss + eta * regularization_term\n",
    "    return total_loss\n",
    "\n",
    "# 定义正则化项（这里可以根据具体需求定义）\n",
    "def regularization_term(model):\n",
    "    # 这里以参数的 L2 范数作为正则化项\n",
    "    reg_term = 0\n",
    "    for param in model.parameters():\n",
    "        reg_term += torch.norm(param)**2\n",
    "    return reg_term\n",
    "\n",
    "# 设置参数\n",
    "eta = 0.1  # 正则化项的权重\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# 创建模型实例\n",
    "model = CNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 模拟数据，这里随机生成，你需要根据具体情况进行调整\n",
    "J = torch.randn(208, 208)\n",
    "V = torch.randn(208, 1)\n",
    "rho = torch.randn(1, 1, 64, 64)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播\n",
    "    output = model(rho)\n",
    "    # 计算正则化项\n",
    "    reg_term = regularization_term(model)\n",
    "    # 计算损失\n",
    "    loss = loss_function(output, J.mm(output.view(-1, 1)), reg_term, eta)\n",
    "    \n",
    "    # 反向传播与优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 打印训练信息\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 208]' is invalid for input of size 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 49>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m output \u001b[38;5;241m=\u001b[39m model(rho)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m208\u001b[39;49m\u001b[43m)\u001b[49m, J), V, regularization_term(model), eta)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 反向传播与优化\u001b[39;00m\n\u001b[0;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 208]' is invalid for input of size 4096"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义神经网络模型\n",
    "class DeepImagePrior(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepImagePrior, self).__init__()\n",
    "        # 在这里定义你的神经网络结构\n",
    "        # 这里只是一个简单的示例\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# 定义损失函数\n",
    "def loss_function(output, target, regularization_term, eta):\n",
    "    reconstruction_loss = torch.norm(output - target)**2\n",
    "    total_loss = reconstruction_loss + eta * regularization_term\n",
    "    return total_loss\n",
    "\n",
    "# 定义正则化项\n",
    "def regularization_term(model):\n",
    "    reg_term = 0\n",
    "    for param in model.parameters():\n",
    "        reg_term += torch.norm(param)**2\n",
    "    return reg_term\n",
    "\n",
    "# 设置参数\n",
    "eta = 0.1  # 正则化项的权重\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# 创建模型实例\n",
    "model = DeepImagePrior()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 模拟数据，这里随机生成，你需要根据具体情况进行调整\n",
    "J = torch.randn(208, 1)  # 调整 J 的形状为 (208, 1)\n",
    "V = torch.randn(208, 1)\n",
    "rho = torch.randn(1, 1, 64, 64)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播\n",
    "    output = model(rho)\n",
    "    # 计算损失\n",
    "    loss = loss_function(torch.matmul(output.view(-1, 208), J), V, regularization_term(model), eta)\n",
    "    \n",
    "    # 反向传播与优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 打印训练信息\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% 定义神经网络模型\n",
    "layers = [\n",
    "    imageInputLayer([64 64 1]) % 输入层，64x64 的单通道图像\n",
    "    convolution2dLayer(3, 16, 'Padding', 'same') % 卷积层\n",
    "    reluLayer % ReLU 激活函数\n",
    "    convolution2dLayer(3, 32, 'Padding', 'same') % 卷积层\n",
    "    reluLayer % ReLU 激活函数\n",
    "    convolution2dLayer(3, 1, 'Padding', 'same') % 卷积层\n",
    "    ];\n",
    "\n",
    "% 创建网络模型\n",
    "net = assembleNetwork(layers);\n",
    "\n",
    "% 定义损失函数\n",
    "function loss = lossFunction(output, target, regularizationTerm, eta)\n",
    "    % 计算重构误差\n",
    "    reconstructionLoss = norm(output(:) - target(:))^2;\n",
    "    % 计算总损失（重构误差 + 正则化项）\n",
    "    loss = reconstructionLoss + eta * regularizationTerm;\n",
    "end\n",
    "\n",
    "\n",
    "% 设置参数\n",
    "eta = 0.1; % 正则化项的权重\n",
    "options = optimoptions(@fminunc, 'Display', 'iter', 'MaxIterations', 100);\n",
    "\n",
    "% 模拟数据，这里随机生成，你需要根据具体情况进行调整\n",
    "J = randn(208, 1);\n",
    "V = randn(208, 1);\n",
    "rho = randn(64, 64);\n",
    "\n",
    "% 训练循环\n",
    "theta0 = net.LearnableParameters;\n",
    "[thetaOpt, loss] = fminunc(@(theta) computeLoss(theta, rho, J, eta), theta0, options);\n",
    "\n",
    "% 最优网络参数\n",
    "net.LearnableParameters = thetaOpt;\n",
    "\n",
    "% 输出损失\n",
    "disp(['Final Loss: ', num2str(loss)]);\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
